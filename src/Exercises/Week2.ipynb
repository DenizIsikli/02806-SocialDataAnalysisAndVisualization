{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "As explained in Lecture 1, each week of this class is a Jupyter notebook like this one. In order to follow the class, you simply start reading from the top, following the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "\n",
    "This lecture covers:\n",
    "\n",
    "1. **Principles of good visualization**: We'll watch a video on what makes plots effective and create your own checklist.\n",
    "2. **Merging messy real-world datasets**: You'll combine SF crime data from two different time periods (2003-2018 and 2018-present) that use different schemas and categories. This is one of the most common and frustrating tasks in data science.\n",
    "3. **Temporal pattern analysis**: Using your merged 20+ year dataset, you'll explore crime patterns across years, months, weekdays, and hours.\n",
    "4. **Advanced visualization techniques**: You'll experiment with calendar plots, polar charts, and time series visualizations.\n",
    "\n",
    "By the end, you'll have a unified crime dataset spanning two decades and the skills to wrangle similarly messy data in your future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating nice plots\n",
    "\n",
    "Ok. There's a lot of barcharts today. We need them ... they are a fantastic tool for data exploration. But it can get monotonous, so let's take a little break to talk about something else before digging deeper with the barcharts.\n",
    "\n",
    "I want to tell you a bit about how to make nice plots. I do that in the video below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python -1.-1.-1)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '//wsl.localhost/Debian/home/deniz/Code/02806-SocialDataAnalysisAndVisualization/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('9hIu4pgJXQc', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.1**: Nice plots\n",
    ">\n",
    "> * Create a list of 10 rules for nice plots based on the video.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> Answer in your own words, do your own research, don't use your LLM. Once you've written down your answers, it's OK to use the LLM to refine your writing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Merging Historical and Recent Crime Data\n",
    "\n",
    "So far we've been working with crime data from 2018 onwards (I will assume you have that file, named something like \"Police Department Incident Reports 2018 to Present\"). But San Francisco has published crime data going all the way back to 2003! Having access to this longer time series would let us:\n",
    "- Study long-term trends in crime\n",
    "- See how crime patterns changed before, during, and after major events\n",
    "- Have more statistical power for our analyses\n",
    "\n",
    "The catch? **The two datasets don't have the same structure.** This is an incredibly common situation in data science. Organizations change how they record data over time, different departments use different formats, and merging datasets requires careful thought.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "In the real world, you'll almost never get a single, clean dataset that answers your question. Instead, you'll need to combine data from multiple sources: different time periods, different departments, different organizations. Each source will have its own quirks, naming conventions, and missing values.\n",
    "\n",
    "This is also directly relevant to algorithmic accountability. Remember the Richardson et al. reading from Week 1? They argued that \"dirty data\" undermines predictive policing. But what counts as \"dirty\"? If crime categories change over time—if `DRUNKENNESS` disappears as a category in 2018—does that mean public intoxication stopped? Or just that we can't track it anymore? These are the kinds of questions you need to ask whenever you merge datasets.\n",
    "\n",
    "The skills you'll practice here—schema mapping, category matching, validation—are foundational for any serious data work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Understand both schemas\n",
    "\n",
    "> **Mini Exercise**: Schema exploration\n",
    ">\n",
    "> Download the historical dataset \"Police Department Incident Reports: Historical 2003 to May 2018\" from SF OpenData (or use the file provided).\n",
    ">\n",
    "> * Load both datasets and examine their columns\n",
    "> * Which columns appear in both datasets (perhaps with different names)?\n",
    "> * Which columns are unique to each dataset?\n",
    "> * Create a \"mapping\" showing which columns in the historical data correspond to which columns in the recent data\n",
    "\n",
    "Here's a starter to help you think about the mapping:\n",
    "\n",
    "| Information | Historical Column | Recent Column |\n",
    "|-------------|------------------|---------------|\n",
    "| Crime type | `Category` | `Incident Category` |\n",
    "| Police district | `PdDistrict` | `Police District` |\n",
    "| Date | `Date` | `Incident Date` |\n",
    "| ... | ... | ... |\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> You should be able to do this no problem in pandas, but if you find it tough, feel free to ask an LLM for help.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Identify the common ground\n",
    "\n",
    "> **Mini Exercise**: Essential columns\n",
    ">\n",
    "> * For the analyses we've have been doing (crime categories, time patterns, police districts), and *will be doing in future lectures* (**mapping GPS points**) which columns do we need?\n",
    "> * Make a list of the essential/potentially interesting columns and their names in each dataset\n",
    "> * Check the data types — are dates stored the same way? Are coordinates in the same format?\n",
    "> * Handle any format differences (hint: look carefully at how dates are formatted in each file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: The Category Problem\n",
    "\n",
    "This is where things get tricky (but interesting). The crime categories in the two datasets are **not the same**.\n",
    "\n",
    "> **Mini Exercise**: Explore the category differences\n",
    ">\n",
    "> * List all unique categories in each dataset\n",
    "> * How many categories are in the historical data? How many in the recent data?\n",
    "> * Which categories appear to match (even if the names are slightly different)?\n",
    "> * Which categories exist in one dataset but not the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: The many flavors of category mismatch\n",
    "\n",
    "Here are some concrete problems you'll discover:\n",
    "\n",
    "**Problem 1: Case and formatting differences**\n",
    "\n",
    "The historical data uses ALL CAPS (`ASSAULT`, `BURGLARY`) while the recent data uses Title Case (`Assault`, `Burglary`). This is easy to fix with `.str.upper()` or `.str.lower()`.\n",
    "\n",
    "**Problem 2: Name changes**\n",
    "\n",
    "Some categories look like they were simply renamed:\n",
    "- `VEHICLE THEFT` → `Motor Vehicle Theft`\n",
    "\n",
    "**Problem 3: Categories that split or merged**\n",
    "\n",
    "This is trickier. For example:\n",
    "- The historical `SEX OFFENSES` might correspond to multiple recent categories: `Sex Offense`, `Rape`, `Human Trafficking`\n",
    "- The recent data has both `Drug Offense` and `Drug Violation` — should these both map to the historical `DRUG/NARCOTIC`?\n",
    "- `VANDALISM` in historical data — is that `Vandalism` or `Malicious Mischief` in the recent data? Or both?\n",
    "\n",
    "**Problem 4: Categories that disappeared or appeared**\n",
    "- `DRUNKENNESS` exists in historical data but not in recent — was it merged into something else? Decriminalized? Maybe Google it or ask an LLM.\n",
    "- `Traffic Collision` exists in recent data but not historical — is this a new category, or was it previously recorded differently?\n",
    "- `Human Trafficking` appears in recent data — is this genuinely new, or was it previously categorized under something else?\n",
    "\n",
    "**Problem 5: Ambiguous mappings**\n",
    "\n",
    "Sometimes it's genuinely unclear how categories should map. Does historical `WEAPON LAWS` correspond to recent `Weapons Offense`, `Weapons Carrying Etc`, or both? Without detailed documentation (which often doesn't exist), you have to make judgment calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Solving the category problem\n",
    "\n",
    "There are two ways that I can think of to approach this matching problem:\n",
    "\n",
    "**Approach A: Manual inspection**\n",
    "\n",
    "Look at both lists carefully and create mappings based on your judgment. \n",
    "\n",
    "**Approach B: Use an LLM to help**\n",
    "\n",
    "LLMs have broad background knowledge about crime categories, legal terminology, and how classification systems work. I think that they might be surprisingly helpful here.\n",
    "\n",
    "> **Exercise 2.1**: LLM-assisted category matching\n",
    ">\n",
    "> Try asking an LLM to help you match categories. Here's a suggested approach:\n",
    ">\n",
    "> 1. Give the LLM both lists of categories\n",
    "> 2. Ask it to suggest which categories likely correspond to each other\n",
    "> 3. Ask it to explain *why* it thinks certain categories match (this helps you evaluate its suggestions)\n",
    "> 4. Ask it to flag any ambiguous cases where human judgment is needed\n",
    ">\n",
    "> **Important**: Don't blindly trust the LLM's suggestions! Use them as a starting point, then verify as described below.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> This is a <i>great</i> use case for LLMs — they have background knowledge about how crime categories work and can spot likely matches faster than manual inspection. But remember: the LLM is making educated guesses based on names. Always verify its suggestions against actual data when possible (see below).\n",
    "</div>\n",
    "\n",
    "#### The crucial last step: Data-driven validation\n",
    "\n",
    "Even after creating a mapping, you should validate it:\n",
    "- For categories you've matched, do the counts make sense? If `VANDALISM` mapped to `Malicious Mischief`, do you see roughly similar numbers of incidents per year?\n",
    "- Look at the transition year (2018) — if your mapping is correct, the yearly/monthly trend should make sense. A sudden jump or drop suggests a mapping problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Define your Personal Focus Crimes\n",
    "\n",
    "Given all these complications, it's time to make some decisions. You need to define a set of **Personal Focus Crimes** — categories that you can confidently track across both datasets.\n",
    "\n",
    "> **Exercise 2.2**: Define Personal Focus Crimes\n",
    ">\n",
    "> Create a list of crime categories that:\n",
    "> 1. Exist in both datasets (possibly with different names)\n",
    "> 2. Have a mapping between the two naming systems that you trust based on checking trendlines\n",
    "> 3. Are interesting enough to analyze (not too rare, not just \"Other\")\n",
    ">\n",
    "> For each Personal Focus Crime, document:\n",
    "> - The name you'll use in your merged dataset\n",
    "> - The corresponding category name in the historical data\n",
    "> - The corresponding category name(s) in the recent data\n",
    "> - Your confidence level (high/medium/low) in the mapping\n",
    "> - Any notes or caveats\n",
    ">\n",
    "> **Example format:**\n",
    ">\n",
    "> | Personal Focus Crime | Historical | Recent | Confidence | Notes |\n",
    "> |---------------------|------------|--------|------------|-------|\n",
    "> | Assault | ASSAULT | Assault | High | Direct match |\n",
    "> | Burglary | BURGLARY | Burglary | High | Direct match |\n",
    "> | Drug Offense | DRUG/NARCOTIC | Drug Offense, Drug Violation | Medium | Merged two recent categories |\n",
    "> | Vehicle Theft | VEHICLE THEFT | Motor Vehicle Theft | High | Name change only |\n",
    "> | ... | ... | ... | ... | ... |\n",
    ">\n",
    "> It's completely fine if your Personal Focus Crimes list is shorter than our original focus crimes list. **Quality over quantity** — it's better to have 8 crimes you can track confidently than 16 where half are questionable.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> Use your LLM to help identify good candidates and spot potential issues, but <b>you</b> need to make the final decisions about what goes on your list. These are judgment calls that affect all your downstream analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6: Merge and validate\n",
    "\n",
    "> **Exercise 2.3**: Now you're ready to construct your final dataset and put the pieces from above together! Some of the key elements are:\n",
    ">\n",
    "> * Standardize column names across both datasets\n",
    "> * Apply your category mappings to create a unified category column\n",
    "> * Concatenate the two datasets\n",
    "> * Remove any duplicate records (there may be overlap around May 2018 — check for incidents that appear in both files)\n",
    "> * **Save your merged dataset for use in future analyses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mini Exercise**: Quick visual overview to validate your merge\n",
    ">\n",
    "> Create a set of line plots showing yearly counts for each of your Personal Focus Crimes from 2003-2025 (or whatever your date range is).\n",
    ">\n",
    "> * Does 2018 look reasonable? The transition between datasets should be make sense\n",
    "> * For any suspicious patterns, document what you found and how you addressed it (or why you left it as-is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7: Think back on your work in this long and epic run of exercises.\n",
    "\n",
    "> **Exercise 2.4**: Reflect on your work\n",
    ">\n",
    "> * What assumptions did you have to make during this merge?\n",
    "> * Which of your Personal Focus Crimes are you most/least confident about? Why?\n",
    "> * If someone used your merged dataset without reading your documentation, what mistakes might they make?\n",
    "> * How does this experience connect to the \"dirty data\" concept from the Richardson *et al.* reading in Week 1?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> The reflection should be your own thinking. This is where you practice the critical data science reasoning that separates good analysts from people who just run code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncle Sune Rants**: This run of exercises and mini exercises in part 2 migth feel frustrating and difficult. If yes, that's awesome! That's the point. You're getting stronger, smarter, and ready for the real world.\n",
    "\n",
    "In the real world, you almost never get perfectly clean, perfectly formatted data handed to you on a silver platter. The ability to wrangle messy, inconsistent data into a usable form is one of the most valuable skills in data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing patterns in the data\n",
    "\n",
    "Visualizing data is a powerful technique that helps us exploit the human eye and make complex patterns easier to identify. \n",
    "\n",
    "Let's use our new mega dataset to look for interesting patterns in the big crime-data. Only consider your Personal Focus Crimes (see Exercise 2.2 above). We'll be generating lots of plots, remember to add good axes, use subplots, pack the plots closely, etc.\n",
    "\n",
    "* **Hint**: You may use this exercise as an extra validation of the personal focus crimes, and I won't be mad if you go back to Exercise 2.2/2.3 and update your list after having done this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.1**: More temporal patterns\n",
    ">\n",
    "> Last time we plotted the development over time (how each of the focus crimes changed over time, year-by-year). Today we'll start by looking at the developments across the months, weekdays, and across the 24 hours of the day. \n",
    ">\n",
    "> * *Yearly trends*. Redo the plots from last week with the new dataset. Do you see anything that surprises you?\n",
    "> * *Weekly patterns*. Basically, we'll forget about the yearly variation and just count up what happens during each weekday (Monday, Tuesday, ...). \n",
    "> * *The months*. We can also check if some months are worse by counting up number of crimes in Jan, Feb, ..., Dec. Did you see any surprises there?\n",
    "> * *The 24 hour cycle*. We can also forget about weekday and simply count up the number of each crime-type that occurs in the dataset from midnight to 1am, 1am - 2am ... and so on. Again: Give me a couple of comments on what you see. \n",
    "> * *Hours of the week*. But by looking at just 24 hours, we may be missing some important trends that can be modulated by week-day, so let's also check out the 168 hours of the week. So let's see the number of each crime-type Monday night from midnight to 1am, Monday night from 1am-2am - all the way to Sunday night from 11pm to midnight.\n",
    ">    * What does this plot tell you about the limitations of your \"24 hour cycle\" plot from above.\n",
    ">    * For some of the crime-types with lots of data, try taking your binning to the 5-minute level. What does that show you? \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> It's OK to ask your LLM for help with creating these plots, <b>but try on your own first</b>. Use the LLM as a kind of TA to help you get your code to work. Don't just ask for answers and pre-written code. By actually doing the counting and slicing yourself, you'll start to get a feel for how to work with temporal data in pandas. That's a key part of becoming a data whisperer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring other types of plots for temporal data\n",
    "\n",
    "We now take a mini-break from barcharts and exploring more ways to plot temporal data.\n",
    "\n",
    "> **Exercise 4.1**: Other cool ways to plot temporal data\n",
    ">\n",
    "> I am going to introduce four different plot-types. Then your job is to choose a part of the crime-data that you care about and plot it using these new ways of visualizing data. I recommend that you choose a different part of the crime-data for each plot-type.\n",
    ">\n",
    "> * **Calendar plots**. Get started on calendar plots **[here](https://calplot.readthedocs.io/en/latest/)**. There are other packages for plotting these, those are also OK to use.\n",
    "> * **Polar bar chart**. Here I want you to plot a 24-hour pattern of some sort — those work really well in radial plots (another name for polar plots) because the day wraps around on itself. You can also try plotting data with patterns from the 168 hours of the week. There's not one super-awesome solution here, you can try using [pure matplotlib](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_bar.html) ... [some examples here](https://www.python-graph-gallery.com/circular-barplot/) or via [plotly](https://plotly.com/python/polar-chart/) (scroll down a bit for the polar barchart).\n",
    "> * **Time series**. Time series is a key functionality of Pandas. To get started, check out the [pandas time series documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html) and the guide on [plotting with a DatetimeIndex](https://pandas.pydata.org/docs/user_guide/visualization.html#plotting-with-a-datetimeindex). For resampling data to different time frequencies (daily, weekly, monthly), see [DataFrame.resample()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html).\n",
    "> * **Heatmaps**. A heatmap showing hour-of-day vs day-of-week is a classic way to visualize temporal patterns. This pairs nicely with your 168-hour analysis from Exercise 3.1. Get started with [seaborn.heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html) — you'll need to pivot your data into a matrix first.\n",
    "> * **Reflection**: What did you learn from using LLMs to solve these visualization challenges?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>LLM guidance:</b> For this exercise, you may use the full power of your LLM! Go nuts and see if you can get these plots working by fully relying on LLM-generated code. This is a deliberate contrast to the previous exercises — afterward, reflect on what was different about this experience.\n",
    "</div>\n",
    "\n",
    "**Note**: I added this exercise with fewer hints than usual. Normally, I help you break down the problems into small solvable bits. Since this is an LLM-heavy exercise, I haven't done that. But I encourage you to play with figuring out the steps yourself — and compare your breakdown to what the LLM suggests.\n",
    "\n",
    "This ties into a bigger picture thing about data science.\n",
    "\n",
    "*My philosophy for data science is this*: Getting to what you want rarely seems hard once you found your way there, the difficulty comes in breaking down a hard problem into the little steps you need to take to solve your complex problem. In this class, I usually do the breaking down for you and provide you with the steps (that's how you go from nothing to creating complex visualizations of crime-data). But I also want you to learn the breaking-problems-down part. LLMs can help here - they can tell you how to break down problems into smaller bits - but if you don't try yourself, you'll never get good at it. So always create your own breakdown first; then see what the computer thinks.\n",
    "\n",
    "*My approach is always to think something along these lines*: Even if my task seems impossible, I think: \"is there any problem that I ***CAN*** solve that will get me closer to where I want to go?\" Once I've solved that part, I'm smarter and I try to think: \"Is there a new problem I can solve that'll get me closer knowing what I know now?\" And I just keep going. Usually that's enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## You've Completed Week 2!\n",
    "\n",
    "This week you tackled one of the messiest but most important skills in data science: merging imperfect datasets. You now have:\n",
    "\n",
    "- A unified crime dataset spanning 2003 to the present\n",
    "- Experience with schema mapping and category matching\n",
    "- A documented set of Personal Focus Crimes that you can track confidently over time\n",
    "- New visualization techniques (calendar plots, polar charts, time series)\n",
    "\n",
    "**Key takeaways:**\n",
    "- Real-world data is messy. Categories change, formats differ, and documentation is often missing.\n",
    "- The assumptions you make during data cleaning directly affect what conclusions you can draw.\n",
    "- Validation is crucial — always check that your merged data makes sense at the boundaries.\n",
    "- Understanding *why* data looks the way it does is just as important as knowing *how* to process it.\n",
    "\n",
    "If you want to go further:\n",
    "- Explore spatial patterns in your merged dataset — which neighborhoods show the most change over 20 years? Do any neighborhoods have different patterns than the city as a whole?\n",
    "- Try different category mappings and see how they affect your conclusions\n",
    "- Look for \"structural breaks\" — points where the data changes in ways that might reflect policy changes, not actual crime changes\n",
    "- Compare your Personal Focus Crimes list with a classmate's — did you make different choices? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
